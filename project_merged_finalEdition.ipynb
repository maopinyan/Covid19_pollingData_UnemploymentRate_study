{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"project_merged_finalEdition.ipynb","provenance":[],"collapsed_sections":["N1ux0_WWEuk4"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DZAg5qggEuk3"},"source":["### UnemploymentRate Data Process from Jan-2020 to Oct-2020"]},{"cell_type":"markdown","metadata":{"id":"JvGLyKWPrNFp"},"source":["Use Pandas to import two files consisting of raw data spanning from August 2019 to September 2020 and preleminary data for October 2020 (downloaded from [Bureau of Labor Statistics](https://www.bls.gov/lau/#tables )) into DataFrame objects.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"5F0O90NcEuk3"},"source":["import pandas as pd\n","import numpy as np\n","from matplotlib import pylab as plt\n","from scipy.interpolate import interp1d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ld9wsnoEuk3"},"source":["### UnemploymentRate, from Jan-2020 to Sep-2020\n","df1 = pd.read_excel('./data/unemploymentRate.xlsx')   ### file 1\n","df_1 = df1[['Unnamed: 1','Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 7', 'Unnamed: 8' ]].drop([0,1,2,3,4])\n","df_1.columns = ['State Code','State Abbreviation','Period','Labor Force','Unemployed','Unemployment Rate']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kRIL-hYpEuk3"},"source":["### UnemploymentRate, Oct-2020\n","df2 = pd.read_excel ('./data/unemploymentRate_Oct.xlsx')    ### file 2\n","df_2 = df2[['State and area', 'Unnamed: 4', 'Unnamed: 8', 'Unnamed: 12']].drop([0,1,2])\n","df_2.columns =['State Abbreviation','Labor Force','Unemployed','Unemployment Rate']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b6GsY6Fodg4b"},"source":["Define a function that takes a state code and returns the DataFrame object containing the semimonthly data of labor force, unemployment and unemployment rate of the state. The function aggregates the state labor force and unemployment data from Jan-20 to Sep-20 and calculates the state unemployment rate. The function obtains the state labor force and unemployment data, and unemployment rate in Oct-20 from the data file. The semimonthly data is obtained via linear interpolation of the monthly data."]},{"cell_type":"code","metadata":{"id":"Fy_Me84ZEuk3"},"source":["date = [\"Jan-20\", \"Feb-20\", \"Mar-20\", \"Apr-20\", \"May-20\", \"Jun-20\", \"Jul-20\", \"Aug-20\", \"Sep-20 p\"]\n","date_new = [\"Jan-20\", \"Feb-20\", \"Mar-20\", \"Apr-20\", \"May-20\", \"Jun-20\", \"Jul-20\", \"Aug-20\", \"Sep-20\", \"Oct-20\"] ### oct-20 in second csv file\n","state_list = {'06':'California', '36':'New York', '12':'Florida', '42':'Pennsylvania', '55':'Wisconsin', '13':'Georgia', '48':'Texas'}\n","my_month = [1, 1, 2, 2 ,3 ,3 ,4 ,4 ,5 ,5 ,6, 6, 7, 7, 8, 8, 9, 9, 10,]\n","my_segment = ['1-1', '1-2', '2-1', '2-2' ,'3-1' ,'3-2' ,'4-1' ,'4-2' ,'5-1' ,'5-2' ,'6-1', '6-2', '7-1', '7-2', '8-1', '8-2', '9-1', '9-2', '10-1',]\n","\n","x = np.linspace(1, 10, num=10, endpoint=True)\n","xnew = np.linspace(1, 10, num=19, endpoint=True)  ### build new dataframe (new period to half month), 19 rows for each state"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UqcSKjNqEuk3"},"source":["############  function: clean data for each state, \n","############  put all needed data into a new dataframe\n","\n","def states_rate(state_code):\n","    ### de_new: put 7 states data into df_new dataframe\n","    df_new = df_1.loc[df_1['State Code'] == state_code]  \n","    ### df_states : build a new dataframe, put data in the new dataframe,\n","    ###             keep only three columns\n","    df_states = pd.DataFrame(np.zeros((10,3)), columns = [ \"Labor Force\", \"Unemployed\", \"Unemployment Rate (%)\"])   \n","    \n","    state = [state_list[state_code]]*len(xnew)  ### state: output the column \"State\", e.g. california \n","    df_states_new = pd.DataFrame()\n","    \n","    for line in range(len(df_new)):\n","        for counter in range(len(date)):\n","            if df_new.iloc[line][\"Period\"] == date[counter]:  ### if \"Period\" column is Jan, Feb, etc\n","                df_states[\"Labor Force\"][counter] += df_new.iloc[line][\"Labor Force\"]    ### sum the column \"Labor Force\"\n","                df_states[\"Unemployed\"][counter] += df_new.iloc[line][\"Unemployed\"]      ### sum the column \"Unemployed\"\n","    \n","    df_oct_states = df_2.loc[df_2['State Abbreviation'] == state_list[state_code]]\n","    df_states[\"Labor Force\"][9] = df_oct_states[\"Labor Force\"]\n","    df_states[\"Unemployed\"][9] =df_oct_states[\"Unemployed\"]\n","   \n","    for counter in range(len(date_new)):   ### get unemployment rate each month\n","        df_states[\"Unemployment Rate (%)\"][counter] = round(df_states[\"Unemployed\"][counter]/df_states[\"Labor Force\"][counter]*100, 2)\n","    #df_states[\"States\"] = state\n","    #df_states[\"Period\"] = date_new\n","    for header in df_states.iloc[:,0:3].keys():  \n","        y = df_states[header]\n","        f = interp1d(x, y)\n","        fnew = f(xnew)\n","        df_states_new[header] = fnew\n","\n","    df_states_new[\"States\"] = state\n","    df_states_new['month'] = my_month\n","    df_states_new[\"Segment\"] = my_segment\n","    return df_states_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SFdlMojIEuk3"},"source":["df_unemployment = pd.concat([states_rate(\"06\"),states_rate(\"55\"),states_rate(\"36\"), states_rate(\"12\"),\n","           states_rate(\"42\"),states_rate(\"13\"), states_rate(\"48\")], axis=0).reset_index().drop(['index'], axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rO_J0MhHEuk3","outputId":"09130936-1a23-49e9-a758-c17a04489662"},"source":["df_unemployment = df_unemployment[['States', 'Labor Force', 'Unemployed', 'Unemployment Rate (%)', 'Segment', 'month']]\n","df_unemployment"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>States</th>\n","      <th>Labor Force</th>\n","      <th>Unemployed</th>\n","      <th>Unemployment Rate (%)</th>\n","      <th>Segment</th>\n","      <th>month</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>California</td>\n","      <td>19477354.0</td>\n","      <td>839987.0</td>\n","      <td>4.310</td>\n","      <td>1-1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>California</td>\n","      <td>19502919.5</td>\n","      <td>841174.0</td>\n","      <td>4.310</td>\n","      <td>1-2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>California</td>\n","      <td>19528485.0</td>\n","      <td>842361.0</td>\n","      <td>4.310</td>\n","      <td>2-1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>California</td>\n","      <td>19360301.0</td>\n","      <td>974383.5</td>\n","      <td>5.035</td>\n","      <td>2-2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>California</td>\n","      <td>19192117.0</td>\n","      <td>1106406.0</td>\n","      <td>5.760</td>\n","      <td>3-1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>128</th>\n","      <td>Texas</td>\n","      <td>14330373.0</td>\n","      <td>1000259.0</td>\n","      <td>6.980</td>\n","      <td>8-1</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>129</th>\n","      <td>Texas</td>\n","      <td>14269358.5</td>\n","      <td>1088924.0</td>\n","      <td>7.635</td>\n","      <td>8-2</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>130</th>\n","      <td>Texas</td>\n","      <td>14208344.0</td>\n","      <td>1177589.0</td>\n","      <td>8.290</td>\n","      <td>9-1</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>131</th>\n","      <td>Texas</td>\n","      <td>14151318.0</td>\n","      <td>1063940.5</td>\n","      <td>7.515</td>\n","      <td>9-2</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>132</th>\n","      <td>Texas</td>\n","      <td>14094292.0</td>\n","      <td>950292.0</td>\n","      <td>6.740</td>\n","      <td>10-1</td>\n","      <td>10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>133 rows Ã— 6 columns</p>\n","</div>"],"text/plain":["         States  Labor Force  Unemployed  Unemployment Rate (%) Segment  month\n","0    California   19477354.0    839987.0                  4.310     1-1      1\n","1    California   19502919.5    841174.0                  4.310     1-2      1\n","2    California   19528485.0    842361.0                  4.310     2-1      2\n","3    California   19360301.0    974383.5                  5.035     2-2      2\n","4    California   19192117.0   1106406.0                  5.760     3-1      3\n","..          ...          ...         ...                    ...     ...    ...\n","128       Texas   14330373.0   1000259.0                  6.980     8-1      8\n","129       Texas   14269358.5   1088924.0                  7.635     8-2      8\n","130       Texas   14208344.0   1177589.0                  8.290     9-1      9\n","131       Texas   14151318.0   1063940.5                  7.515     9-2      9\n","132       Texas   14094292.0    950292.0                  6.740    10-1     10\n","\n","[133 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"Kd0QXVQBEuk4"},"source":["### Polling Data Process from Jan-2020 to Oct-2020\n","\n","*   List item\n","*   List item\n","\n"]},{"cell_type":"markdown","metadata":{"id":"R4VrGAZwUwmv"},"source":["Used Pandas library to read csv source file 'president_polls.csv that downloaded from FiveThirtyEight website, please save this source file in the same directory, folder named 'Election_preference_polls_data' specified parse_dates argument for easy handle of dates of column 'start_date' and 'end_date'. Imported warning module to ignore warnings.\n"]},{"cell_type":"code","metadata":{"id":"AL0VYH3GEuk4"},"source":["import warnings\n","import pandas as pd\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7RiZ401AEuk4"},"source":["data = pd.read_csv(\"./data/president_polls.csv\", parse_dates = [\"start_date\", \"end_date\"])\n","#data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kAyylCFRWu2i"},"source":["Parsed data and only retain colunmns of 'state', 'start_date'(start date of a pollster data), 'answer'(each candidates), and 'pct'(percentage of each candidates)."]},{"cell_type":"code","metadata":{"id":"Td667V_eEuk4"},"source":["data_refined = data[[\"state\",\"start_date\",\"answer\",\"pct\"]]\n","#data_refined.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zxn5sk6BXJq9"},"source":["Drop any non-value cells by .dropna()."]},{"cell_type":"code","metadata":{"id":"Hk13Z_eqEuk4"},"source":["data_refined = data_refined.dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b9kYbi72XaTh"},"source":["Restricted column of state to only retain seven key states that we are focusing on and filtered candidates from column 'answer' to only Trump and Biden."]},{"cell_type":"code","metadata":{"id":"upqruW9pEuk4"},"source":["answer_lst = [\"Biden\", \"Trump\"]\n","state_lst = [\"Pennsylvania\", \"New York\", \"Florida\", \"California\", \"Georgia\", \"Texas\", \"Wisconsin\"]\n","data_refined = data_refined[data_refined[\"answer\"].isin(answer_lst)]\n","data_refined = data_refined[data_refined[\"state\"].isin(state_lst)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c8z814btXx0G"},"source":["Used pandas Series.dt.year which is the year of the datetime module to filter the column 'start_date' to only retain year is 2020."]},{"cell_type":"code","metadata":{"id":"-qbCrpBsEuk4"},"source":["data_refined = data_refined[(data_refined['start_date'].dt.year == 2020)]\n","#data_refined.shape\n","#data_refined.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HpSsqXksYLEB"},"source":["For easy handling of data, seperated dataframe into two, one is where 'answer' column is Trump and another one is Biden. Sorted values using .sort_values() by 'state' column first, then the 'start_date' column. "]},{"cell_type":"code","metadata":{"id":"knYZFSteEuk4"},"source":["data_Trump = data_refined[(data_refined['answer']=='Trump')]\n","data_Biden = data_refined[(data_refined['answer']=='Biden')]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cFEuyNArEuk4"},"source":["data_Trump.sort_values(by=['state','start_date'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ea4o1MCDEuk4"},"source":["data_Biden.sort_values(by=['state','start_date'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nUT9ZgBrY8BW"},"source":["Created columns of 'year', 'month', 'day' by datetime methods. Renamed 'pct' column to 'Trump(%)' and 'Biden(%)'  respectively. Dropped column 'answer' and 'start_date'."]},{"cell_type":"code","metadata":{"id":"RRhZHBE0Euk4"},"source":["data_Trump['year'] = data_Trump['start_date'].dt.year\n","data_Trump['month'] = data_Trump['start_date'].dt.month\n","data_Trump['day'] = data_Trump['start_date'].dt.day\n","data_Trump = data_Trump.rename(columns = {\"pct\": \"Trump(%)\"})\n","data_Trump = data_Trump.drop(columns = [\"answer\", \"start_date\"])\n","#data_Trump.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SCTvUTdNEuk4"},"source":["data_Biden['year'] = data_Biden['start_date'].dt.year\n","data_Biden['month'] = data_Biden['start_date'].dt.month\n","data_Biden['day'] = data_Biden['start_date'].dt.day\n","data_Biden = data_Biden.rename(columns = {\"pct\": \"Biden(%)\"})\n","data_Biden = data_Biden.drop(columns = [\"answer\", \"start_date\"])\n","\n","#data_Biden.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NqZN-FuLZxiB"},"source":["Created a column of 'Segment' which describes whether each row's day fall in first half of month or second half of month. segment is denoted like '1-1' for first half of January and '1-2' denotes second half of Janaury. "]},{"cell_type":"code","metadata":{"id":"4fgAKBz1Euk4"},"source":["segment = []\n","for day in data_Trump[\"day\"]:\n","    if day <= 15:\n","        segment.append(\"-1\")\n","    else:\n","        segment.append(\"-2\")\n","data_Trump['segment'] = segment\n","#data_Trump.head()   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5zmim94SEuk4"},"source":["segment = []\n","for day in data_Biden[\"day\"]:\n","    if day <= 15:\n","        segment.append(\"-1\")\n","    else:\n","        segment.append(\"-2\")\n","data_Biden['segment'] = segment\n","#data_Biden.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z6JBrzMLEuk4"},"source":["data_Trump['Segment'] = data_Trump['month'].astype(str) + data_Trump['segment']\n","data_Trump = data_Trump.drop(['segment','day'], axis = 1)\n","#data_Trump.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZgZIwH-Euk4"},"source":["data_Biden['Segment'] = data_Biden['month'].astype(str) + data_Biden['segment']\n","data_Biden = data_Biden.drop(['segment','day'], axis = 1)\n","#data_Biden.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1boJlWBImgr3"},"source":["Seperated data_Trump and data_Biden each into seven sub-dataframe, and then for each sub-dataframe, used groupby on 'Segment' and compute the mean of each candidate's percentage. The mean of each corresponding segment is stored in the new column: 'Trump (%)', annd 'Biden (%)'. Note there is a space in this column name that differs from another column. Then merge the groupby object back to sub-dataframe. Now we have each candidate's dataframe seperate into seven states, and each segment, which is on half month base, has its own value. In another word, half month's average of percentage of each candidates in each states is calculated. "]},{"cell_type":"code","metadata":{"id":"3hk3hQ7GHSo4"},"source":["ca_Trump = data_Trump[(data_Trump['state'] == 'California')]\n","pa_Trump = data_Trump[(data_Trump['state'] == 'Pennsylvania')]\n","ny_Trump = data_Trump[(data_Trump['state'] == 'New York')]\n","fl_Trump = data_Trump[(data_Trump['state'] == 'Florida')]\n","ga_Trump = data_Trump[(data_Trump['state'] == 'Georgia')]\n","tx_Trump = data_Trump[(data_Trump['state'] == 'Texas')]\n","wi_Trump = data_Trump[(data_Trump['state'] == 'Wisconsin')]\n","\n","ca_Biden = data_Biden[(data_Biden['state'] == 'California')]\n","pa_Biden = data_Biden[(data_Biden['state'] == 'Pennsylvania')]\n","ny_Biden = data_Biden[(data_Biden['state'] == 'New York')]\n","fl_Biden = data_Biden[(data_Biden['state'] == 'Florida')]\n","ga_Biden = data_Biden[(data_Biden['state'] == 'Georgia')]\n","tx_Biden = data_Biden[(data_Biden['state'] == 'Texas')]\n","wi_Biden = data_Biden[(data_Biden['state'] == 'Wisconsin')]\n","# ca_Biden.head(30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rMJYiUHNHSh8"},"source":["Trump_state_lst = [ca_Trump, pa_Trump, ny_Trump, fl_Trump, ga_Trump, tx_Trump, wi_Trump]\n","Biden_state_lst = [ca_Biden, pa_Biden, ny_Biden, fl_Biden, ga_Biden, tx_Biden, wi_Biden]\n","\n","tavg_ca = ca_Trump.groupby('Segment')[['Trump(%)']].mean().rename(columns = {'Trump(%)':'Trump (%)'}) \n","tavg_pa = pa_Trump.groupby('Segment')[['Trump(%)']].mean().rename(columns = {'Trump(%)':'Trump (%)'}) \n","tavg_ny = ny_Trump.groupby('Segment')[['Trump(%)']].mean().rename(columns = {'Trump(%)':'Trump (%)'}) \n","tavg_fl = fl_Trump.groupby('Segment')[['Trump(%)']].mean().rename(columns = {'Trump(%)':'Trump (%)'}) \n","tavg_ga = ga_Trump.groupby('Segment')[['Trump(%)']].mean().rename(columns = {'Trump(%)':'Trump (%)'}) \n","tavg_tx = tx_Trump.groupby('Segment')[['Trump(%)']].mean().rename(columns = {'Trump(%)':'Trump (%)'}) \n","tavg_wi = wi_Trump.groupby('Segment')[['Trump(%)']].mean().rename(columns = {'Trump(%)':'Trump (%)'}) \n","\n","bavg_ca = ca_Biden.groupby('Segment')[['Biden(%)']].mean().rename(columns = {'Biden(%)':'Biden (%)'}) \n","bavg_pa = pa_Biden.groupby('Segment')[['Biden(%)']].mean().rename(columns = {'Biden(%)':'Biden (%)'}) \n","bavg_ny = ny_Biden.groupby('Segment')[['Biden(%)']].mean().rename(columns = {'Biden(%)':'Biden (%)'}) \n","bavg_fl = fl_Biden.groupby('Segment')[['Biden(%)']].mean().rename(columns = {'Biden(%)':'Biden (%)'}) \n","bavg_ga = ga_Biden.groupby('Segment')[['Biden(%)']].mean().rename(columns = {'Biden(%)':'Biden (%)'}) \n","bavg_tx = tx_Biden.groupby('Segment')[['Biden(%)']].mean().rename(columns = {'Biden(%)':'Biden (%)'}) \n","bavg_wi = wi_Biden.groupby('Segment')[['Biden(%)']].mean().rename(columns = {'Biden(%)':'Biden (%)'}) \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PYA9MbHRHSV6","outputId":"ad128992-f6b1-4b47-d3f2-02c5595aaac0"},"source":["ca_Trump = pd.merge(ca_Trump, tavg_ca, how = 'inner', on = 'Segment')  \n","pa_Trump = pd.merge(pa_Trump, tavg_pa, how = 'inner', on = 'Segment')\n","ny_Trump = pd.merge(ny_Trump, tavg_ny, how = 'inner', on = 'Segment')\n","fl_Trump = pd.merge(fl_Trump, tavg_fl, how = 'inner', on = 'Segment')\n","ga_Trump = pd.merge(ga_Trump, tavg_ga, how = 'inner', on = 'Segment')\n","tx_Trump = pd.merge(tx_Trump, tavg_tx, how = 'inner', on = 'Segment')\n","wi_Trump = pd.merge(wi_Trump, tavg_wi, how = 'inner', on = 'Segment')\n","\n","ca_Biden = pd.merge(ca_Biden, bavg_ca, how = 'inner', on = 'Segment')  \n","pa_Biden = pd.merge(pa_Biden, bavg_pa, how = 'inner', on = 'Segment')\n","ny_Biden = pd.merge(ny_Biden, bavg_ny, how = 'inner', on = 'Segment')\n","fl_Biden = pd.merge(fl_Biden, bavg_fl, how = 'inner', on = 'Segment')\n","ga_Biden = pd.merge(ga_Biden, bavg_ga, how = 'inner', on = 'Segment')\n","tx_Biden = pd.merge(tx_Biden, bavg_tx, how = 'inner', on = 'Segment')\n","wi_Biden = pd.merge(wi_Biden, bavg_wi, how = 'inner', on = 'Segment')\n","\n","for i in Trump_state_lst:\n","    i.sort_values(by=['state','month'], inplace=True)\n","\n","for i in Biden_state_lst:\n","    i.sort_values(by=['state','month'], inplace=True)\n","ca_Biden.head(20)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>state</th>\n","      <th>Biden(%)</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>Segment</th>\n","      <th>Biden (%)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>California</td>\n","      <td>59.00</td>\n","      <td>2020</td>\n","      <td>1</td>\n","      <td>1-1</td>\n","      <td>59.000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>California</td>\n","      <td>59.90</td>\n","      <td>2020</td>\n","      <td>2</td>\n","      <td>2-1</td>\n","      <td>58.450</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>California</td>\n","      <td>57.00</td>\n","      <td>2020</td>\n","      <td>2</td>\n","      <td>2-1</td>\n","      <td>58.450</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>California</td>\n","      <td>57.80</td>\n","      <td>2020</td>\n","      <td>2</td>\n","      <td>2-2</td>\n","      <td>59.725</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>California</td>\n","      <td>60.00</td>\n","      <td>2020</td>\n","      <td>2</td>\n","      <td>2-2</td>\n","      <td>59.725</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>California</td>\n","      <td>62.10</td>\n","      <td>2020</td>\n","      <td>2</td>\n","      <td>2-2</td>\n","      <td>59.725</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>California</td>\n","      <td>59.00</td>\n","      <td>2020</td>\n","      <td>2</td>\n","      <td>2-2</td>\n","      <td>59.725</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>California</td>\n","      <td>67.00</td>\n","      <td>2020</td>\n","      <td>3</td>\n","      <td>3-2</td>\n","      <td>67.000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>California</td>\n","      <td>65.17</td>\n","      <td>2020</td>\n","      <td>5</td>\n","      <td>5-1</td>\n","      <td>65.170</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>California</td>\n","      <td>57.00</td>\n","      <td>2020</td>\n","      <td>5</td>\n","      <td>5-2</td>\n","      <td>57.500</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>California</td>\n","      <td>58.00</td>\n","      <td>2020</td>\n","      <td>5</td>\n","      <td>5-2</td>\n","      <td>57.500</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>California</td>\n","      <td>62.19</td>\n","      <td>2020</td>\n","      <td>6</td>\n","      <td>6-1</td>\n","      <td>62.540</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>California</td>\n","      <td>62.89</td>\n","      <td>2020</td>\n","      <td>6</td>\n","      <td>6-1</td>\n","      <td>62.540</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>California</td>\n","      <td>63.10</td>\n","      <td>2020</td>\n","      <td>7</td>\n","      <td>7-1</td>\n","      <td>63.345</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>California</td>\n","      <td>63.59</td>\n","      <td>2020</td>\n","      <td>7</td>\n","      <td>7-1</td>\n","      <td>63.345</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>California</td>\n","      <td>67.00</td>\n","      <td>2020</td>\n","      <td>7</td>\n","      <td>7-2</td>\n","      <td>67.000</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>California</td>\n","      <td>63.00</td>\n","      <td>2020</td>\n","      <td>8</td>\n","      <td>8-1</td>\n","      <td>60.820</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>California</td>\n","      <td>63.28</td>\n","      <td>2020</td>\n","      <td>8</td>\n","      <td>8-1</td>\n","      <td>60.820</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>California</td>\n","      <td>61.00</td>\n","      <td>2020</td>\n","      <td>8</td>\n","      <td>8-1</td>\n","      <td>60.820</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>California</td>\n","      <td>56.00</td>\n","      <td>2020</td>\n","      <td>8</td>\n","      <td>8-1</td>\n","      <td>60.820</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         state  Biden(%)  year  month Segment  Biden (%)\n","0   California     59.00  2020      1     1-1     59.000\n","1   California     59.90  2020      2     2-1     58.450\n","2   California     57.00  2020      2     2-1     58.450\n","3   California     57.80  2020      2     2-2     59.725\n","4   California     60.00  2020      2     2-2     59.725\n","5   California     62.10  2020      2     2-2     59.725\n","6   California     59.00  2020      2     2-2     59.725\n","7   California     67.00  2020      3     3-2     67.000\n","8   California     65.17  2020      5     5-1     65.170\n","9   California     57.00  2020      5     5-2     57.500\n","10  California     58.00  2020      5     5-2     57.500\n","11  California     62.19  2020      6     6-1     62.540\n","12  California     62.89  2020      6     6-1     62.540\n","13  California     63.10  2020      7     7-1     63.345\n","14  California     63.59  2020      7     7-1     63.345\n","15  California     67.00  2020      7     7-2     67.000\n","16  California     63.00  2020      8     8-1     60.820\n","17  California     63.28  2020      8     8-1     60.820\n","18  California     61.00  2020      8     8-1     60.820\n","19  California     56.00  2020      8     8-1     60.820"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"Ly-kci7sHSKI"},"source":["state_lst = [\"Pennsylvania\", \"New York\", \"Florida\", \"California\", \"Georgia\", \"Texas\", \"Wisconsin\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0vF7FqhaOeFE"},"source":["Deleted duplicates in the data set because of previous step. Concatenated each candidates in seven states back to one dataframe. Dropped previous each single date's percentage column,'Trump(%)' and 'Biden(%)'"]},{"cell_type":"code","metadata":{"id":"pQYBtf64HR-L"},"source":["ca_Trump.drop_duplicates(subset = 'Segment', keep = \"first\", inplace = True)\n","pa_Trump.drop_duplicates(subset = 'Segment', keep = \"first\", inplace = True)\n","ny_Trump.drop_duplicates(subset = 'Segment', keep = \"first\", inplace = True)\n","fl_Trump.drop_duplicates(subset = 'Segment', keep = \"first\", inplace = True)\n","ga_Trump.drop_duplicates(subset = 'Segment', keep = \"first\", inplace = True)\n","tx_Trump.drop_duplicates(subset = 'Segment', keep = \"first\", inplace = True)\n","wi_Trump.drop_duplicates(subset = 'Segment', keep = \"first\", inplace = True)\n","\n","ca_Biden.drop_duplicates(subset = 'Segment', keep = \"first\", inplace = True)\n","pa_Biden.drop_duplicates(subset = 'Segment', keep = \"first\", inplace = True)\n","ny_Biden.drop_duplicates(subset = 'Segment', keep = \"first\", inplace = True)\n","fl_Biden.drop_duplicates(subset = 'Segment', keep = \"first\", inplace = True)\n","ga_Biden.drop_duplicates(subset = 'Segment', keep = \"first\", inplace = True)\n","tx_Biden.drop_duplicates(subset = 'Segment', keep = \"first\", inplace = True)\n","wi_Biden.drop_duplicates(subset = 'Segment', keep = \"first\", inplace = True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j_zQuh1iHRyN"},"source":["frames_Biden = [ca_Biden,fl_Biden, ga_Biden, ny_Biden, pa_Biden, tx_Biden, wi_Biden]\n","frames_Trump = [ca_Trump,fl_Trump, ga_Trump, ny_Trump, pa_Trump, tx_Trump, wi_Trump]\n","\n","Trump_df = pd.concat(frames_Trump)\n","Trump_df = Trump_df.drop('Trump(%)', axis = 1)   \n","\n","Biden_df = pd.concat(frames_Biden)\n","Biden_df = Biden_df.drop('Biden(%)', axis = 1) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3HZOCjRXuWIF"},"source":["Combine two dataframes of Trump's and Biden's into one dataframe, Adjust column names to aligh with unemployment rate dataframes and COVID19 dataframes."]},{"cell_type":"code","metadata":{"id":"9klX_NhbEuk4","outputId":"de0640ad-7ef2-467d-c71a-0aaceb2c9a28"},"source":["df_polling_data = pd.merge(Trump_df, Biden_df, how = 'outer') \n","df_polling_data.rename(columns = {'state' : \"States\"}, inplace = True)\n","df_polling_data['year'] = df_polling_data['year'].astype(str)\n","df_polling_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>States</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>Segment</th>\n","      <th>Trump (%)</th>\n","      <th>Biden (%)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>California</td>\n","      <td>2020</td>\n","      <td>1</td>\n","      <td>1-1</td>\n","      <td>37.000000</td>\n","      <td>59.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>California</td>\n","      <td>2020</td>\n","      <td>2</td>\n","      <td>2-1</td>\n","      <td>34.275000</td>\n","      <td>58.450000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>California</td>\n","      <td>2020</td>\n","      <td>2</td>\n","      <td>2-2</td>\n","      <td>31.577273</td>\n","      <td>59.725000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>California</td>\n","      <td>2020</td>\n","      <td>3</td>\n","      <td>3-2</td>\n","      <td>29.000000</td>\n","      <td>67.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>California</td>\n","      <td>2020</td>\n","      <td>5</td>\n","      <td>5-1</td>\n","      <td>34.830000</td>\n","      <td>65.170000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>113</th>\n","      <td>Wisconsin</td>\n","      <td>2020</td>\n","      <td>8</td>\n","      <td>8-2</td>\n","      <td>42.334231</td>\n","      <td>50.783077</td>\n","    </tr>\n","    <tr>\n","      <th>114</th>\n","      <td>Wisconsin</td>\n","      <td>2020</td>\n","      <td>9</td>\n","      <td>9-1</td>\n","      <td>43.417692</td>\n","      <td>50.777308</td>\n","    </tr>\n","    <tr>\n","      <th>115</th>\n","      <td>Wisconsin</td>\n","      <td>2020</td>\n","      <td>9</td>\n","      <td>9-2</td>\n","      <td>42.942500</td>\n","      <td>53.102500</td>\n","    </tr>\n","    <tr>\n","      <th>116</th>\n","      <td>Wisconsin</td>\n","      <td>2020</td>\n","      <td>10</td>\n","      <td>10-1</td>\n","      <td>43.668182</td>\n","      <td>51.404545</td>\n","    </tr>\n","    <tr>\n","      <th>117</th>\n","      <td>Wisconsin</td>\n","      <td>2020</td>\n","      <td>10</td>\n","      <td>10-2</td>\n","      <td>43.728571</td>\n","      <td>52.140000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>118 rows Ã— 6 columns</p>\n","</div>"],"text/plain":["         States  year  month Segment  Trump (%)  Biden (%)\n","0    California  2020      1     1-1  37.000000  59.000000\n","1    California  2020      2     2-1  34.275000  58.450000\n","2    California  2020      2     2-2  31.577273  59.725000\n","3    California  2020      3     3-2  29.000000  67.000000\n","4    California  2020      5     5-1  34.830000  65.170000\n","..          ...   ...    ...     ...        ...        ...\n","113   Wisconsin  2020      8     8-2  42.334231  50.783077\n","114   Wisconsin  2020      9     9-1  43.417692  50.777308\n","115   Wisconsin  2020      9     9-2  42.942500  53.102500\n","116   Wisconsin  2020     10    10-1  43.668182  51.404545\n","117   Wisconsin  2020     10    10-2  43.728571  52.140000\n","\n","[118 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"N1ux0_WWEuk4"},"source":["### Covid_19 Data Process from Jan-2020 to Oct-2020"]},{"cell_type":"code","metadata":{"id":"cpSOlJCMEuk5"},"source":["from operator import itemgetter\n","import csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i26CDl8UEuk5"},"source":["workpath = \"./data/\"\n","covid_frame = pd.read_csv(\"./data/3_cases_and_deaths_by_state_timeseries.csv\")\n","#state_study = [\"California\", \"New York\", \"Texas\", \"Georgia\", \"Florida\", \"Pennsylvania\", \"Wisconsin\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E4BsgiOPEuk5"},"source":["# input raw polls data as dataframe\n","covid_raw = covid_frame.values.tolist()\n","\n","covid_daily_7states = []\n","for row in covid_raw:\n","    if row[0] in [\"California\", \"New York\", \"Texas\", \"Georgia\", \"Florida\", \"Pennsylvania\", \"Wisconsin\"]:\n","        date = row[1].split(\"-\")  # [year, month, day]\n","        segment = 1 if int(date[2]) <= 15 else 2\n","        #print(row)\n","        new_case = 0 if row[7] == \"\" else float(row[7])\n","        new_case_1e6 = 0.0 if row[12] == \"\" else float(row[12])\n","        new_death = 0 if row[9] == \"\" else float(row[9])\n","        new_death_1e6 = 0.0 if row[11] == \"\" else float(row[11])\n","        covid_short = [row[0], date[0], date[1], segment, new_case, new_case_1e6, new_death, new_death_1e6]\n","        #print(covid_short)\n","        covid_daily_7states.append(covid_short)\n","\n","covid_daily_7states = sorted(covid_daily_7states, key = itemgetter(0,1,2,3,))  \n","#covid_daily_7states[900:950]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RUsT8WxVEuk5"},"source":["covid = covid_daily_7states[:]\n","covid_15d_7states = []\n","\n","# initializing\n","sum_newcase = covid[0][4]\n","sum_newcase_1e6 = covid[0][5]\n","sum_newdeath = covid[0][6]\n","sum_newdeath_1e6 = covid[0][7]\n","count_day = 1\n","for i in range(1, len(covid)):\n","    if (covid[i][0] == covid[i-1][0]) and (covid[i][1] == covid[i-1][1]) and (covid[i][2] == covid[i-1][2]) and (covid[i][3] == covid[i-1][3]):\n","        sum_newcase += covid[i][4]\n","        sum_newcase_1e6 += covid[i][5]\n","        sum_newdeath += covid[i][6]\n","        sum_newdeath_1e6 += covid[i][7]\n","        count_day += 1\n","    else:\n","        #print(sum_newcase,sum_newcase_1e6,sum_newdeath,sum_newdeath_1e6,count_day)\n","        aver_newcase = float(sum_newcase / count_day)\n","        aver_newcase_1e6 = float(sum_newcase_1e6 / count_day)\n","        aver_newdeath = float(sum_newdeath / count_day)\n","        aver_newdeath_1e6 = float(sum_newdeath_1e6 / count_day)\n","\n","        sum_newcase = covid[i][4]\n","        sum_newcase_1e6 = covid[i][5]\n","        sum_newdeath = covid[i][6]\n","        sum_newdeath_1e6 = covid[i][7]\n","        count_day = 1\n","\n","        covid_15d = covid[i][0:4] + [aver_newcase, aver_newcase_1e6, aver_newdeath, aver_newdeath_1e6]\n","        #print(covid_15d)\n","        covid_15d_7states.append(covid_15d)\n","\n","# seperate the data for each state\n","covid_15d_ca = []\n","covid_15d_ny = []\n","covid_15d_ga = []\n","covid_15d_tx = []\n","covid_15d_fl = []\n","covid_15d_pa = []\n","covid_15d_wi = []\n","for record in covid_15d_7states:\n","    if record[0] == \"California\":\n","        covid_15d_ca.append(record)\n","    elif record[0] == \"New York\":\n","        covid_15d_ny.append(record)\n","    elif record[0] == \"Texas\":\n","        covid_15d_tx.append(record)\n","    elif record[0] == \"Georgia\":\n","        covid_15d_ga.append(record)\n","    elif record[0] == \"Florida\":\n","        covid_15d_fl.append(record)\n","    elif record[0] == \"Pennsylvania\":\n","        covid_15d_pa.append(record)\n","    elif record[0] == \"Wisconsin\":\n","        covid_15d_wi.append(record)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u5FrBWc3Euk5"},"source":["# output the 15d polls as a csv file for California\n","ca_file_15d = workpath + \"covid_15d_California.csv\"\n","with open(ca_file_15d, 'w', newline='') as csvfile:\n","    csvwriter = csv.writer(csvfile)\n","    for row in covid_15d_ca:\n","        csvwriter.writerow(row)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dBGX6lYFEuk5"},"source":["ny_file_15d = workpath + \"covid_15d_New York.csv\"\n","with open(ny_file_15d, 'w', newline='') as csvfile:\n","    csvwriter = csv.writer(csvfile)\n","    for row in covid_15d_ny:\n","        csvwriter.writerow(row)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"egR3FQtPEuk5"},"source":["tx_file_15d = workpath + \"covid_15d_Texas.csv\"\n","with open(tx_file_15d, 'w', newline='') as csvfile:\n","    csvwriter = csv.writer(csvfile)\n","    for row in covid_15d_tx:\n","        csvwriter.writerow(row)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ImTF56IvEuk5"},"source":["ga_file_15d = workpath + \"covid_15d_Georgia.csv\"\n","with open(ga_file_15d, 'w', newline='') as csvfile:\n","    csvwriter = csv.writer(csvfile)\n","    for row in covid_15d_ga:\n","        csvwriter.writerow(row)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1UJA9-bLEuk5"},"source":["fl_file_15d = workpath + \"covid_15d_Florida.csv\"\n","with open(fl_file_15d, 'w', newline='') as csvfile:\n","    csvwriter = csv.writer(csvfile)\n","    for row in covid_15d_fl:\n","        csvwriter.writerow(row)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qXRiX6diEuk5"},"source":["pa_file_15d = workpath + \"covid_15d_Pennsylvania.csv\"\n","with open(pa_file_15d, 'w', newline='') as csvfile:\n","    csvwriter = csv.writer(csvfile)\n","    for row in covid_15d_pa:\n","        csvwriter.writerow(row)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xx68F3neEuk5"},"source":["wi_file_15d = workpath + \"covid_15d_Wisconsin.csv\"\n","with open(wi_file_15d, 'w', newline='') as csvfile:\n","    csvwriter = csv.writer(csvfile)\n","    for row in covid_15d_wi:\n","        csvwriter.writerow(row)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jUGYgkMhEuk5"},"source":["CA = pd.read_csv(workpath + \"covid_15d_California.csv\", names=['States', 'year', 'month', 'Segment',\n","                             'new_case', 'new_case_per_1e6', 'new_death',  'new_death_per_1e6'])\n","FL = pd.read_csv(workpath + \"covid_15d_Florida.csv\", names=['States', 'year', 'month', 'Segment',\n","                             'new_case', 'new_case_per_1e6', 'new_death',  'new_death_per_1e6'])\n","GA = pd.read_csv(workpath + \"covid_15d_Georgia.csv\", names=['States', 'year', 'month', 'Segment',\n","                             'new_case', 'new_case_per_1e6', 'new_death',  'new_death_per_1e6'])\n","NY = pd.read_csv(workpath + \"covid_15d_New York.csv\", names=['States', 'year', 'month', 'Segment',\n","                             'new_case', 'new_case_per_1e6', 'new_death',  'new_death_per_1e6'])\n","PA = pd.read_csv(workpath + \"covid_15d_Pennsylvania.csv\", names=['States', 'year', 'month', 'Segment',\n","                             'new_case', 'new_case_per_1e6', 'new_death',  'new_death_per_1e6'])\n","TX = pd.read_csv(workpath + \"covid_15d_Texas.csv\", names=['States', 'year', 'month', 'Segment',\n","                             'new_case', 'new_case_per_1e6', 'new_death',  'new_death_per_1e6'])\n","WI = pd.read_csv(workpath + \"covid_15d_Wisconsin.csv\", names=['States', 'year', 'month', 'Segment',\n","                             'new_case', 'new_case_per_1e6', 'new_death',  'new_death_per_1e6'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V9aDozxpEuk5","executionInfo":{"elapsed":707,"status":"aborted","timestamp":1606792881982,"user":{"displayName":"Zifeng Wang","photoUrl":"","userId":"15635417509856900087"},"user_tz":300},"outputId":"d292adca-406a-418f-ed63-9e9588052760"},"source":["df_covid = pd.concat([CA, FL, GA, NY, PA, TX, WI], axis=0).reset_index().drop(\"index\", axis = 1)\n","df_covid.rename(columns = {'state' : \"States\", 'segment': 'Segment'}, inplace = True)\n","df_covid"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>States</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>Segment</th>\n","      <th>new_case</th>\n","      <th>new_case_per_1e6</th>\n","      <th>new_death</th>\n","      <th>new_death_per_1e6</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>California</td>\n","      <td>2020</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>California</td>\n","      <td>2020</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0.333333</td>\n","      <td>0.000667</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>California</td>\n","      <td>2020</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0.285714</td>\n","      <td>0.000714</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>California</td>\n","      <td>2020</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>24.066667</td>\n","      <td>0.062000</td>\n","      <td>0.333333</td>\n","      <td>0.001333</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>California</td>\n","      <td>2020</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>491.250000</td>\n","      <td>1.241875</td>\n","      <td>10.500000</td>\n","      <td>0.027500</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>141</th>\n","      <td>Wisconsin</td>\n","      <td>2020</td>\n","      <td>9</td>\n","      <td>2</td>\n","      <td>1046.733333</td>\n","      <td>18.004000</td>\n","      <td>6.133333</td>\n","      <td>0.104667</td>\n","    </tr>\n","    <tr>\n","      <th>142</th>\n","      <td>Wisconsin</td>\n","      <td>2020</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>2064.666667</td>\n","      <td>35.515333</td>\n","      <td>8.266667</td>\n","      <td>0.141333</td>\n","    </tr>\n","    <tr>\n","      <th>143</th>\n","      <td>Wisconsin</td>\n","      <td>2020</td>\n","      <td>10</td>\n","      <td>2</td>\n","      <td>2670.066667</td>\n","      <td>45.928667</td>\n","      <td>19.533333</td>\n","      <td>0.334667</td>\n","    </tr>\n","    <tr>\n","      <th>144</th>\n","      <td>Wisconsin</td>\n","      <td>2020</td>\n","      <td>11</td>\n","      <td>1</td>\n","      <td>4721.500000</td>\n","      <td>81.213750</td>\n","      <td>40.250000</td>\n","      <td>0.692500</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>Wisconsin</td>\n","      <td>2020</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>6137.000000</td>\n","      <td>105.564000</td>\n","      <td>27.466667</td>\n","      <td>0.473333</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>146 rows Ã— 8 columns</p>\n","</div>"],"text/plain":["         States  year  month  Segment     new_case  new_case_per_1e6  \\\n","0    California  2020      2        1          NaN               NaN   \n","1    California  2020      2        2     0.333333          0.000667   \n","2    California  2020      3        1     0.285714          0.000714   \n","3    California  2020      3        2    24.066667          0.062000   \n","4    California  2020      4        1   491.250000          1.241875   \n","..          ...   ...    ...      ...          ...               ...   \n","141   Wisconsin  2020      9        2  1046.733333         18.004000   \n","142   Wisconsin  2020     10        1  2064.666667         35.515333   \n","143   Wisconsin  2020     10        2  2670.066667         45.928667   \n","144   Wisconsin  2020     11        1  4721.500000         81.213750   \n","145   Wisconsin  2020     11        2  6137.000000        105.564000   \n","\n","     new_death  new_death_per_1e6  \n","0          NaN                NaN  \n","1     0.000000           0.000000  \n","2     0.000000           0.000000  \n","3     0.333333           0.001333  \n","4    10.500000           0.027500  \n","..         ...                ...  \n","141   6.133333           0.104667  \n","142   8.266667           0.141333  \n","143  19.533333           0.334667  \n","144  40.250000           0.692500  \n","145  27.466667           0.473333  \n","\n","[146 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"zmVBEqCyEuk5"},"source":["df_covid[\"Segment\"] = df_covid[\"month\"].astype(str) + '-' + df_covid[\"Segment\"].astype(str)\n","#df_covid"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TyipAYLiEuk5"},"source":["### Merge three dataset"]},{"cell_type":"markdown","metadata":{"id":"1vDlSuO4M7xI"},"source":["Merge three dataset: Unemployment Rate data, Polling data and Covid_19 data. Perform the outer join on three columns(Segment, States, month)\n","First, merge df_unemployment and df_polling_data."]},{"cell_type":"code","metadata":{"id":"W9G0ktfhEuk5"},"source":["##### df_polling_data and df_unemployment\n","df_merge_Polling_Unemployment = pd.merge(df_unemployment, df_polling_data, \n","                         how = 'outer', on = ['Segment', 'States', 'month'])\n","\n","#df_merge_Polling_Unemployment"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IBc5dmyWOT-K"},"source":["Second, merge the third dataframe(df_covid). We will get a full dataframe with three datasets."]},{"cell_type":"code","metadata":{"id":"x1Af5WmwEuk5"},"source":["######## merge three dataframe\n","df_merge_all = pd.merge(df_merge_Polling_Unemployment, df_covid, \n","                         how = 'outer', on = ['States', 'month', 'Segment'])\n","\n","df_merge_all.drop('year_y', axis = 1)\n","df_merge_all.rename(columns = {'year_x' : \"year\"}, inplace = True)\n","\n","df_merge_all = df_merge_all[['States', 'year', 'month', 'Segment',\n","                             'Labor Force', 'Unemployed', 'Unemployment Rate (%)', \n","                             'Trump (%)', 'Biden (%)',\n","                             'new_case', 'new_case_per_1e6', 'new_death',  'new_death_per_1e6']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kNXTr7NuOypo"},"source":["Change the column names, make it much easier to read. "]},{"cell_type":"code","metadata":{"id":"-4EpmoG6Euk5","outputId":"8062ab0b-08eb-45ab-d9a4-ebff829c14d9"},"source":["##### format dataframe\n","df_merge_all.rename(columns = {'new_case' : \"New Case\", 'new_case_per_1e6' : 'New Case Per 10$^6$', \n","                               'new_death' : 'New Death', 'new_death_per_1e6' : 'New Death Per 10$^6$'}, inplace = True)\n","df_merge_all['Trump (%)'] = round(df_merge_all['Trump (%)'] , 2)\n","df_merge_all['Biden (%)'] = round(df_merge_all['Biden (%)'] , 2)\n","df_merge_all['New Case'] = round(df_merge_all['New Case'] , 4)\n","df_merge_all['New Death'] = round(df_merge_all['New Death'] , 4)\n","df_merge_all['New Case Per 10$^6$'] = round(df_merge_all['New Case Per 10$^6$'] , 5)\n","df_merge_all['New Death Per 10$^6$'] = round(df_merge_all['New Death Per 10$^6$'] , 5)\n","df_merge_all"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>States</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>Segment</th>\n","      <th>Labor Force</th>\n","      <th>Unemployed</th>\n","      <th>Unemployment Rate (%)</th>\n","      <th>Trump (%)</th>\n","      <th>Biden (%)</th>\n","      <th>New Case</th>\n","      <th>New Case Per 10$^6$</th>\n","      <th>New Death</th>\n","      <th>New Death Per 10$^6$</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>California</td>\n","      <td>2020</td>\n","      <td>1</td>\n","      <td>1-1</td>\n","      <td>19477354.0</td>\n","      <td>839987.0</td>\n","      <td>4.310</td>\n","      <td>37.00</td>\n","      <td>59.00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>California</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>1-2</td>\n","      <td>19502919.5</td>\n","      <td>841174.0</td>\n","      <td>4.310</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>California</td>\n","      <td>2020</td>\n","      <td>2</td>\n","      <td>2-1</td>\n","      <td>19528485.0</td>\n","      <td>842361.0</td>\n","      <td>4.310</td>\n","      <td>34.28</td>\n","      <td>58.45</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>California</td>\n","      <td>2020</td>\n","      <td>2</td>\n","      <td>2-2</td>\n","      <td>19360301.0</td>\n","      <td>974383.5</td>\n","      <td>5.035</td>\n","      <td>31.58</td>\n","      <td>59.72</td>\n","      <td>0.3333</td>\n","      <td>0.00067</td>\n","      <td>0.0000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>California</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>3-1</td>\n","      <td>19192117.0</td>\n","      <td>1106406.0</td>\n","      <td>5.760</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.2857</td>\n","      <td>0.00071</td>\n","      <td>0.0000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>149</th>\n","      <td>Pennsylvania</td>\n","      <td>NaN</td>\n","      <td>11</td>\n","      <td>11-2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3727.0000</td>\n","      <td>29.10000</td>\n","      <td>32.6667</td>\n","      <td>0.25467</td>\n","    </tr>\n","    <tr>\n","      <th>150</th>\n","      <td>Texas</td>\n","      <td>NaN</td>\n","      <td>11</td>\n","      <td>11-1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>6059.9375</td>\n","      <td>21.11375</td>\n","      <td>73.6875</td>\n","      <td>0.25688</td>\n","    </tr>\n","    <tr>\n","      <th>151</th>\n","      <td>Texas</td>\n","      <td>NaN</td>\n","      <td>11</td>\n","      <td>11-2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>8818.5333</td>\n","      <td>30.72467</td>\n","      <td>102.1333</td>\n","      <td>0.35533</td>\n","    </tr>\n","    <tr>\n","      <th>152</th>\n","      <td>Wisconsin</td>\n","      <td>NaN</td>\n","      <td>11</td>\n","      <td>11-1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>4721.5000</td>\n","      <td>81.21375</td>\n","      <td>40.2500</td>\n","      <td>0.69250</td>\n","    </tr>\n","    <tr>\n","      <th>153</th>\n","      <td>Wisconsin</td>\n","      <td>NaN</td>\n","      <td>11</td>\n","      <td>11-2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>6137.0000</td>\n","      <td>105.56400</td>\n","      <td>27.4667</td>\n","      <td>0.47333</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>154 rows Ã— 13 columns</p>\n","</div>"],"text/plain":["           States  year  month Segment  Labor Force  Unemployed  \\\n","0      California  2020      1     1-1   19477354.0    839987.0   \n","1      California   NaN      1     1-2   19502919.5    841174.0   \n","2      California  2020      2     2-1   19528485.0    842361.0   \n","3      California  2020      2     2-2   19360301.0    974383.5   \n","4      California   NaN      3     3-1   19192117.0   1106406.0   \n","..            ...   ...    ...     ...          ...         ...   \n","149  Pennsylvania   NaN     11    11-2          NaN         NaN   \n","150         Texas   NaN     11    11-1          NaN         NaN   \n","151         Texas   NaN     11    11-2          NaN         NaN   \n","152     Wisconsin   NaN     11    11-1          NaN         NaN   \n","153     Wisconsin   NaN     11    11-2          NaN         NaN   \n","\n","     Unemployment Rate (%)  Trump (%)  Biden (%)   New Case  \\\n","0                    4.310      37.00      59.00        NaN   \n","1                    4.310        NaN        NaN        NaN   \n","2                    4.310      34.28      58.45        NaN   \n","3                    5.035      31.58      59.72     0.3333   \n","4                    5.760        NaN        NaN     0.2857   \n","..                     ...        ...        ...        ...   \n","149                    NaN        NaN        NaN  3727.0000   \n","150                    NaN        NaN        NaN  6059.9375   \n","151                    NaN        NaN        NaN  8818.5333   \n","152                    NaN        NaN        NaN  4721.5000   \n","153                    NaN        NaN        NaN  6137.0000   \n","\n","     New Case Per 10$^6$  New Death  New Death Per 10$^6$  \n","0                    NaN        NaN                   NaN  \n","1                    NaN        NaN                   NaN  \n","2                    NaN        NaN                   NaN  \n","3                0.00067     0.0000               0.00000  \n","4                0.00071     0.0000               0.00000  \n","..                   ...        ...                   ...  \n","149             29.10000    32.6667               0.25467  \n","150             21.11375    73.6875               0.25688  \n","151             30.72467   102.1333               0.35533  \n","152             81.21375    40.2500               0.69250  \n","153            105.56400    27.4667               0.47333  \n","\n","[154 rows x 13 columns]"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"p84RNSVoOxA0"},"source":["Write the dataframe to a csv file."]},{"cell_type":"code","metadata":{"id":"DO_MfKN4Euk5"},"source":["##### write into a csv file\n","df_merge_all.to_csv(\"./data/final_merged_data.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fYocs9AlEuk5"},"source":[""],"execution_count":null,"outputs":[]}]}